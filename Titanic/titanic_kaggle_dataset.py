# -*- coding: utf-8 -*-
"""Titanic_Kaggle_dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mvo9bcqQcwcAZTj3jkWi6BQ1_EfeP5le

# Predicting whether a person survived or not the titanic


---

1. Problem definition
2. Data
3. Evaluation
4. Features
5. Modelling
6. Experimentation

### 1. Problem definition
Given data about each passenger , predict if they will survive or not

### 2. Data
The data set contains 10 attributes:

* survival
* pclass
* sex
* Age
* sibsp
* parch
* ticket
* fare
* cabin
* embarked

### 3. Evaluation
We need over 90 % accuracy at prediction whether a passenger survived or not

### 4. Featues
- Survival 
  * 0 = no
  * 1 = yes

- pclass: A proxy for socio-economics tatus (SES)
  * 1st = Upper
  * 2nd = Middle
  * 3rd = Lower

- Sex
 * Male
 * Female

- age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5

- sibsp: The dataset defines family relations in this way...
  * Sibling = brother, sister, stepbrother, stepsister
  * Spouse = husband, wife (mistresses and fianc√©s were ignored)

- parch: The dataset defines family relations in this way...
  * Parent = mother, father
  * Child = daughter, son, stepdaughter, stepson
  * Some children travelled only with a nanny, therefore parch=0 for them.

- Ticket
  * Ticket number

- Fare
 * Passenger fair

- Cabin
  * Cabin number

- Embarked
 Port of Embarkation:
 * C = Cherbourg, 
 * Q = Queenstown,
 * S = Southampton


### 5. Moddeling
Most likely to try a RandomForestRegression model and then a Deep learning tensorflow model
"""

# Commented out IPython magic to ensure Python compatibility.
# Import all the tools we need

# Regular EDA (exploratory data analysis) and plotting libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# We want our plots to appear inside the notebook
# %matplotlib inline 

# Modles from Scikit-Learn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

# Model evaluations
from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.model_selection import RandomizedSearchCV,GridSearchCV
from sklearn.metrics import confusion_matrix,classification_report
from sklearn.metrics import precision_score,recall_score,f1_score
from sklearn.metrics import plot_roc_curve

# Set default figsize to 10 inches by 6 inches
plt.rcParams["figure.figsize"] = (10,6)

df = pd.read_csv("/content/drive/MyDrive/My_machine_learning_projects/Titanic/train.csv")
df.head()

df_tmp = df.copy()

len(df_tmp)

df_tmp.shape

df_tmp.isna().sum()

df_tmp["Survived"].value_counts().plot(kind="bar",color=["salmon","lightblue"]);

pd.crosstab(df_tmp["Survived"],df_tmp.Sex).plot(kind="bar",
                                                color=["salmon","lightblue"]);
plt.rcParams.update({'font.size':10})
plt.xticks(rotation=0)
plt.xlabel("0 = did not survive  1 = Survived");

print(pd.pivot_table(df_tmp,index="Survived",
               columns="Pclass",
               values="Ticket",
               aggfunc="count"))
print()
print(pd.pivot_table(df_tmp,index="Survived",
                     columns="Sex",
                     values="Ticket",
                     aggfunc="count"))

df_tmp.describe()

df_tmp.info

# View Numeric data
for label ,content in df_tmp.items():
  if pd.api.types.is_numeric_dtype(content):
    print(label)

# View object data
for label , content in df_tmp.items():
  if pd.api.types.is_object_dtype(content):
    print(label)

def preprocess_data(df):
  # Turn evey loccation in a 0/1 table and then drop the embarked 
  Embarked = []
  for content, label in df.items():
    if content == "Embarked":
      Embarked.append(label)

  df["Cherbourg"] = 0
  df["Queenstown"] = 0
  df["Southampton"] = 0
  df["Not specified"] = 0

  for i in range (len(Embarked)):
    if label[0][i] == "C":
      df["Cherbourg"][i] = 1
    elif label[0][i] == "S":
      df["Southampton"][i] = 1
    elif label[0][i] == "Q":
      df["Queenstown"] = 1
    else:
      df["Not specified"] = 1 

  df.drop("Embarked",axis = 1, inplace=True)

  # Fill Cabin missing data with No cabin parameter
  df["Cabin"] = df["Cabin"].fillna("No Cabin")

  # Fill numeric rows with the median
  for label,content in df_tmp.items():
    if pd.api.types.is_numeric_dtype(content):
      if pd.isnull(content).sum():
        # Add a binary column which tells us if the data was missing
        df[label+"_is_missing"] = pd.isnull(content)
        # Fill missing values with the median
        df[label] = content.fillna(content.mean())

  # Turn categorical variables into numbers and fill missing 
  for label ,content in df.items():
    if not pd.api.types.is_numeric_dtype(content):
      # Add a binary column to indicate whether sample had missing values
      df[label+"_is_missing"] = pd.isnull(content)
      # Turn categories into numbers and add +1
      df[label] = df[label].str.replace(',','','.')
      df[label] = pd.Categorical(content).codes + 1

  # Turn categorical variables into numbers


  return df

df_tmp = preprocess_data(df_tmp)
df_tmp

# Check if there are any null object values
for label , content in df_tmp.items():
  if pd.api.types.is_object_dtype(content):
    if pd.isnull(content).sum():
      print(label)

# Check if there are any null numeric values
for label , content in df_tmp.items():
  if pd.api.types.is_numeric_dtype(content):
    if pd.isnull(content).sum():
      print(label)

df_tmp.dtypes

# Make training sets
X_train = df_tmp.drop("Survived",axis=1)
y_train = df_tmp["Survived"]

# Create a hyperparameter grid for RandomForestClassifier
rf_grid = {"n_estimators": np.arange(10,1000,50),
           "max_depth":[None,3,5,10],
           "min_samples_split":np.arange(2,20,2),
           "min_samples_leaf":np.arange(1,20,2)}

# %%time

# # Set up random hyperparameter search for RandomForestClassifier
# gs_log_reg = RandomizedSearchCV(RandomForestClassifier(n_jobs=-1),
#                           param_distributions=rf_grid,
#                           cv=5,
#                           n_iter=800,
#                           verbose=True)

# gs_log_reg.fit(X_train,y_train)

!pip install catboost

from catboost import CatBoostClassifier,Pool,cv
cat_features = np.where(X_train.dtypes != np.float)[0]
cat_features

# Cat boost train pool
train_pool = Pool(X_train,y_train,cat_features)

cat_model = CatBoostClassifier(iterations=1000,
                               task_type="GPU",
                               loss_function='Logloss',
                               custom_loss=['Accuracy'])

cat_model.fit(train_pool,plot=True)
acc_catboost = round(cat_model.score(X_train, y_train) * 100, 2)

acc_catboost

cv_params = cat_model.get_params()

cv_data = cv(train_pool,
             cv_params,
             fold_count=10,
             plot=True)

acc_cv_catboost = round(np.max(cv_data['test-Accuracy-mean']) * 100, 2)

acc_cv_catboost

# Import test data
df_test = pd.read_csv("/content/drive/MyDrive/My_machine_learning_projects/Titanic/test.csv")
df_test.head()

df_test = preprocess_data(df_test)

set(X_train.columns) - set(df_test.columns)

df_test["Age_is_missing"] = False

X_train

df_test["Age"] = df_test["Age"].fillna(df_test.Age.mean())
df_test["Fare"] = df_test["Fare"].fillna(df_test.Fare.median())

df_test.isna().sum()

test_preds = cat_model.predict(df_test)

df_preds = pd.DataFrame()

df_preds["PassengerId"] = df_test["PassengerId"]
df_preds["Survived"] = test_preds
df_preds

# Export prediction data

df_preds.to_csv("/content/drive/MyDrive/My_machine_learning_projects/Titanic/cat_boost_model_v3.csv",index=False)

